{% set version = "1.9.0" %}
# see github.com/conda-forge/conda-forge.github.io/issues/1059 for naming discussion
{% set faiss_proc_type = "cuda" if cuda_compiler_version != "None" else "cpu" %}

{% if cuda_compiler_version != "None" %}
{% set cuda_major = environ.get("cuda_compiler_version", "11.8").split(".")[0] | int %}
{% else %}
{% set cuda_major = 0 %}
{% set cuda_compiler_version = "None" %}
{% endif %}

# headers for upstream-folders 'faiss/*.h', 'faiss/{impl,invlists,utils}/*.h',
# see https://github.com/facebookresearch/faiss/blob/v{{ version }}/faiss/CMakeLists.txt;
# gpu adds headers in 'faiss/gpu/*.h', 'faiss/gpu/{impl,utils}/*.(cu)?h'.
# generated by:
# ls faiss/ | grep -E "h$"
# ls faiss/gpu/ | grep -E "h$"
{% set headers = [
    'AutoTune.h', 'Clustering.h', 'IVFlib.h', 'Index.h', 'Index2Layer.h',
    'IndexAdditiveQuantizer.h', 'IndexAdditiveQuantizerFastScan.h', 'IndexBinary.h',
    'IndexBinaryFlat.h', 'IndexBinaryFromFloat.h', 'IndexBinaryHNSW.h', 'IndexBinaryHash.h',
    'IndexBinaryIVF.h', 'IndexFastScan.h', 'IndexFlat.h', 'IndexFlatCodes.h', 'IndexHNSW.h',
    'IndexIDMap.h', 'IndexIVF.h', 'IndexIVFAdditiveQuantizer.h', 'IndexIVFAdditiveQuantizerFastScan.h',
    'IndexIVFFastScan.h', 'IndexIVFFlat.h', 'IndexIVFIndependentQuantizer.h', 'IndexIVFPQ.h',
    'IndexIVFPQFastScan.h', 'IndexIVFPQR.h', 'IndexIVFSpectralHash.h', 'IndexLSH.h', 'IndexLattice.h',
    'IndexNNDescent.h', 'IndexNSG.h', 'IndexNeuralNetCodec.h', 'IndexPQ.h', 'IndexPQFastScan.h',
    'IndexPreTransform.h', 'IndexRefine.h', 'IndexReplicas.h', 'IndexRowwiseMinMax.h',
    'IndexScalarQuantizer.h', 'IndexShards.h', 'IndexShardsIVF.h', 'MatrixStats.h', 'MetaIndexes.h',
    'MetricType.h', 'VectorTransform.h', 'clone_index.h', 'index_factory.h', 'index_io.h'
] + (cuda_compiler_version != "None") * [
    'gpu/GpuAutoTune.h', 'gpu/GpuCloner.h', 'gpu/GpuClonerOptions.h', 'gpu/GpuDistance.h',
    'gpu/GpuFaissAssert.h', 'gpu/GpuIcmEncoder.h', 'gpu/GpuIndex.h', 'gpu/GpuIndexBinaryFlat.h',
    'gpu/GpuIndexFlat.h', 'gpu/GpuIndexIVF.h', 'gpu/GpuIndexIVFFlat.h', 'gpu/GpuIndexIVFPQ.h',
    'gpu/GpuIndexIVFScalarQuantizer.h', 'gpu/GpuIndicesOptions.h', 'gpu/GpuResources.h',
    'gpu/StandardGpuResources.h'
] %}

package:
  name: faiss-split
  version: {{ version }}

source:
  url: https://github.com/facebookresearch/faiss/archive/v{{ version }}.tar.gz
  sha256: f6721f1a479b0bdd8547e5bb0dbfa872643e7ea37686c6bf6fc99fb933b3fe4f
  patches:
    - patches/0001-adapt-header-target-directory-to-outputname.patch
    # patch for avoiding crash in GPU test suite on windows
    - patches/0002-skip-test_stress-for-GPU-on-windows.patch
    # enable building libfaiss_avx2 without libfaiss
    - patches/0003-enable-building-libfaiss_avx2-without-libfaiss.patch
    # increase tolerance for test that occasionally fails marginally
    - patches/0004-increase-tolerance-for-marginally-failing-test.patch
    # add /bigobj on windows to avoid: "fatal error C1128: number of sections exceeded object file format limit"
    - patches/0005-add-bigobj-to-swigfaiss-compile-options-on-windows.patch
    # due to switch to ninja
    - patches/0006-no-more-Release-subfolder.patch
    # more fixes
    - patches/0007-fix-index-type-in-openmp-loop.patch
    # compare https://github.com/facebookresearch/faiss/pull/2927
    - patches/0008-loosen-test-tolerance-in-test_ivf_train_2level.patch
    - patches/0009-don-t-hide-faiss-gpu-symbols-temporarily-install-fai.patch   # [win]
    - patches/0010-don-t-rely-on-luck-for-initialization-order-on-win-C.patch   # [win and cuda_compiler_version != "None"]
    # workaround for https://github.com/facebookresearch/faiss/issues/2985
    - patches/0011-fix-a-MSVC-problem.patch                                     # [win and cuda_compiler_version != "None"]
    # backport https://github.com/facebookresearch/faiss/pull/4010 for missing headers
    - patches/0012-Some-chore-fixes-4010.patch
    # disable library dispatch based on detected CPU instruction sets;
    # revisit this after (or when) #23 is fixed
    - patches/0013-disable-loading-different-library-flavours-based-on-.patch

build:
  number: 0

requirements:
  build:
    - {{ stdlib('c') }}
    - {{ compiler('cxx') }}
    - {{ compiler('cuda') }}  # [cuda_compiler_version != "None"]

outputs:
  # A meta-package to select CPU or GPU build for faiss.
  - name: faiss-proc
    version: 1.0.0
    build:
      string: {{ faiss_proc_type }}
    test:
      commands:
        - exit 0

  - name: libfaiss
    script: build-lib.sh    # [unix]
    script: build-lib.bat   # [win]
    build:
      string: "h{{ PKG_HASH }}_{{ PKG_BUILDNUM }}_{{ faiss_proc_type }}"                                                    # [cuda_compiler_version == "None"]
      string: "cuda{{ cuda_compiler_version | replace('.', '') }}h{{ PKG_HASH }}_{{ PKG_BUILDNUM }}_{{ faiss_proc_type }}"  # [cuda_compiler_version != "None"]
      run_exports:
        # faiss follows SemVer, so restrict packages built with libfaiss to use
        # at least the same version at runtime, but below the next major version.
        - libfaiss >={{ version }},<2
        # additionally, we need to ensure matching proc-type
        - libfaiss =*=*_{{ faiss_proc_type }}
    requirements:
      build:
        - {{ stdlib('c') }}
        - {{ compiler('cxx') }}
        - {{ compiler('cuda') }}    # [cuda_compiler_version != "None"]
        # necessary for conda-build to build the python bindings per version
        - python                    # [build_platform != target_platform]
        - cmake
        - ninja
        {% if cuda_major >= 12 %}
        - cuda-version {{ cuda_compiler_version }}  # [build_platform != target_platform]
        - cuda-cudart-dev           # [build_platform != target_platform]
        - cuda-profiler-api         # [build_platform != target_platform]
        - libcublas-dev             # [build_platform != target_platform]
        - libcurand-dev             # [build_platform != target_platform]
        {% endif %}
      host:
        - libgomp                   # [linux]
        - llvm-openmp               # [osx]
        - libblas                   # [not osx]
        - liblapack                 # [not osx]
        {% if cuda_major >= 12 %}
        - cuda-version {{ cuda_compiler_version }}
        - cuda-cudart-dev
        - cuda-profiler-api
        - libcublas-dev
        - libcurand-dev
        {% endif %}
      run_constrained:
        - faiss-cpu <0.0a0  # [cuda_compiler_version != "None"]
        - faiss-gpu <0.0a0  # [cuda_compiler_version == "None"]
        - faiss-proc =*={{ faiss_proc_type }}

    test:
      commands:
        # shared
        - test -f $PREFIX/lib/libfaiss.so               # [linux]
        - test -f $PREFIX/lib/libfaiss.dylib            # [osx]
        - if not exist %LIBRARY_BIN%\faiss.dll exit 1   # [win]
        - if not exist %LIBRARY_LIB%\faiss.lib exit 1   # [win]

        # absence of static libraries
        - test ! -f $PREFIX/lib/libfaiss.a              # [unix]
        - test ! -f $PREFIX/lib/libfaiss_gpu.a          # [unix]
        - if exist %LIBRARY_LIB%\faiss_gpu.lib exit 1   # [win]

        # headers
        {% for each_header in headers %}
        - test -f $PREFIX/include/faiss/{{ each_header }} || (echo "{{ each_header }} not found" && exit 1)  # [unix]
        - if not exist %LIBRARY_INC%\faiss\{{ "\\".join(each_header.split("/")) }} exit 1                    # [win]
        {% endfor %}

        # CMake metadata
        - test -f $PREFIX/lib/cmake/faiss/faiss-targets.cmake                # [unix]
        - if not exist %LIBRARY_LIB%\cmake\faiss\faiss-targets.cmake exit 1  # [win]

  - name: faiss
    script: build-pkg.sh          # [not win]
    script: build-pkg.bat         # [win]
    build:
      string: "py{{ CONDA_PY }}h{{ PKG_HASH }}_{{ PKG_BUILDNUM }}_{{ faiss_proc_type }}"                                                    # [cuda_compiler_version == "None"]
      string: "py{{ CONDA_PY }}cuda{{ cuda_compiler_version | replace('.', '') }}h{{ PKG_HASH }}_{{ PKG_BUILDNUM }}_{{ faiss_proc_type }}"  # [cuda_compiler_version != "None"]
    requirements:
      build:
        - {{ stdlib('c') }}
        - {{ compiler('cxx') }}
        - {{ compiler('cuda') }}              # [cuda_compiler_version != "None"]
        - swig
        - cmake
        - ninja
        - python                              # [build_platform != target_platform]
        - cross-python_{{ target_platform }}  # [build_platform != target_platform]
        - numpy                               # [build_platform != target_platform]
      host:
        - python
        - pip
        - setuptools
        - numpy
        - libfaiss ={{ version }}=*_{{ faiss_proc_type }}
        - libgomp                             # [linux]
        - llvm-openmp                         # [osx]
        {% if cuda_major >= 12 %}
        - cuda-version {{ cuda_compiler_version }}
        - libcublas-dev                       # [win]
        {% endif %}
      run:
        - python
        - packaging
        - libfaiss ={{ version }}=*_{{ faiss_proc_type }}
      run_constrained:
        - faiss-cpu <0.0a0  # [cuda_compiler_version != "None"]
        - faiss-gpu <0.0a0  # [cuda_compiler_version == "None"]
        - faiss-proc =*={{ faiss_proc_type }}

    {% if not (aarch64 or ppc64le) or py == 311 %}
    # only run the full test suite for one python version when in emulation
    # (each run can take up to 5h!); there's essentially zero divergence
    # in behaviour across python versions anyway
    test:
      requires:
        # trying to test all blas-variants runs into conda/conda-build#3947
        # - libblas =*=*{{ blas_impl }}
        # testing with MKL on x86_64, as upstream considers this the most important
        - libblas =*=*mkl  # [x86_64]
        - scipy
        - pytest
      files:
        - test-pkg.bat
        - test-pkg.sh
      source_files:
        - tests/
      imports:
        - faiss
      commands:
        {% set skip = "_not_a_real_test" %}
        # flaky double encoding test that occasionally fails
        {% set skips = "test_RQ6x8" %}
        # TestComputeGT switches between CPU & GPU implementation depending on availability;
        # GPU device detection (on win) currently seems broken in CUDA, and segfaults the test suite
        {% set skips = skips + " or TestComputeGT" %}                                      # [win]
        # test relies on linux-isms
        {% set skips = skips + " or (test_contrib and test_checkpoint)" %}                 # [win]
        # marginal tolerance violation
        {% set skips = skips + " or (TestProductLocalSearchQuantizer and test_lut)" %}     # [osx]
        # two failing tests on on aarch (test_index_accuracy & test_index_accuracy2)
        {% set skips = skips + " or (test_residual_quantizer and test_index_accuracy)" %}  # [aarch64 or ppc64le]
        # test errors on aarch (probably due to emulation)
        {% set skips = skips + " or (TestPCA and test_pca)" %}                             # [aarch64]
        {% set skips = skips + " or test_PQ4_speed" %}                                     # [aarch64]
        {% set skips = skips + " or test_update_codebooks" %}                              # [aarch64]
        # a test run in emulation takes ~2.5h on the fastest agents,
        # and over 5h on a slow one; we need to skip some of the
        # longest-running tests to have a chance of finishing
        {% if aarch64 or ppc64le %}
            # test modules that may take 2h or more on a slow agent
            {% set skips = skips + " or test_index_accuracy.py" %}
            {% set skips = skips + " or test_fast_scan.py" %}
            # other really slow tests
            {% set skips = skips + " or (test_merge_index.py and TestMerge1)" %}
            {% set skips = skips + " or (test_meta_index.py and IDRemap)" %}
            {% set skips = skips + " or (TestClone and AdditiveQuantizer)" %}
            {% set skips = skips + " or (TestIndexProduct and test_accuracy1)" %}
            {% set skips = skips + " or test_wrapped_quantizer_HNSW" %}
        {% endif %}
        # the linux & windows CI agents support AVX2 (OSX doesn't yet), so by default,
        # we expect faiss will load the library with AVX2-support, see
        # https://github.com/facebookresearch/faiss/blob/v1.7.1/faiss/python/loader.py#L52-L66
        - export SKIPS="({{ skips }})" && ./test-pkg.sh  # [unix]
        - set "SKIPS=({{ skips }})"    && test-pkg.bat   # [win]

        # running the following test requires an actual GPU device, which is not available in CI
        # - pytest faiss/gpu/test/
    {% endif %}

  # for compatibility with (& ease of migration from) existing packages in the pytorch channel
  - name: faiss-cpu
    build:
      skip: true  # [cuda_compiler_version != "None"]
    requirements:
      run:
        - faiss ={{ version }}=*_cpu
    test:
      imports:
        - faiss

  - name: faiss-gpu
    build:
      skip: true  # [cuda_compiler_version == "None"]
    requirements:
      run:
        - faiss ={{ version }}=*_cuda
    test:
      imports:
        - faiss

about:
  home: https://github.com/facebookresearch/faiss
  license: MIT
  license_family: MIT
  license_file: LICENSE
  summary: 'A library for efficient similarity search and clustering of dense vectors.'

  description: |
    Faiss is a library for efficient similarity search and clustering of dense vectors.
    It contains algorithms that search in sets of vectors of any size, up to ones that
    possibly do not fit in RAM. It also contains supporting code for evaluation and
    parameter tuning. Faiss is written in C++ with complete wrappers for Python/numpy.
    Some of the most useful algorithms are implemented on the GPU. It is developed by
    [Facebook AI Research](https://research.fb.com/category/facebook-ai-research-fair/).

    For best performance, the maintainers of the package
    [recommend](https://github.com/conda-forge/staged-recipes/pull/11337#issuecomment-623718460)
    using the MKL implementation of blas/lapack. You can ensure that this is installed
    by adding "libblas =*=*mkl" to your dependencies.
  doc_url: https://rawgit.com/facebookresearch/faiss/master/docs/html/annotated.html
  dev_url: https://github.com/facebookresearch/faiss

extra:
  recipe-maintainers:
    - h-vetinari
